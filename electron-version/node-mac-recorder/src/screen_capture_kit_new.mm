#import "screen_capture_kit.h"
#import <CoreImage/CoreImage.h>

static SCStream *g_stream = nil;
static id<SCStreamDelegate> g_streamDelegate = nil;
static id<SCStreamOutput> g_streamOutput = nil;
static BOOL g_isRecording = NO;

// Frame-based approach for working video
static NSMutableArray<NSImage *> *g_capturedFrames = nil;
static NSString *g_outputPath = nil;
static NSInteger g_maxFrames = 150; // 5 seconds at 30fps

@interface FrameCapturDelegate : NSObject <SCStreamDelegate>
@end

@implementation FrameCapturDelegate
- (void)stream:(SCStream *)stream didStopWithError:(NSError *)error {
    NSLog(@"ðŸ›‘ Stream stopped");
    g_isRecording = NO;
    
    // Create video from captured frames
    [ScreenCaptureKitRecorder createVideoFromFrames];
}
@end

@interface FrameCaptureOutput : NSObject <SCStreamOutput>
@end

@implementation FrameCaptureOutput
- (void)stream:(SCStream *)stream didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer ofType:(SCStreamOutputType)type {
    if (!g_isRecording || type != SCStreamOutputTypeScreen) {
        return;
    }
    
    @autoreleasepool {
        CVPixelBufferRef pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer);
        if (pixelBuffer && g_capturedFrames.count < g_maxFrames) {
            
            // Convert pixel buffer to UIImage
            CIContext *context = [CIContext context];
            CIImage *ciImage = [CIImage imageWithCVPixelBuffer:pixelBuffer];
            CGImageRef cgImage = [context createCGImage:ciImage fromRect:ciImage.extent];
            
            if (cgImage) {
                NSImage *image = [[NSImage alloc] initWithCGImage:cgImage size:NSZeroSize];
                [g_capturedFrames addObject:image];
                
                CGImageRelease(cgImage);
                
                if (g_capturedFrames.count % 30 == 0) {
                    NSLog(@"ðŸ“¸ Captured %lu frames", (unsigned long)g_capturedFrames.count);
                }
            }
        }
    }
}
@end

@implementation ScreenCaptureKitRecorder

+ (BOOL)isScreenCaptureKitAvailable {
    if (@available(macOS 12.3, *)) {
        return [SCShareableContent class] != nil && [SCStream class] != nil;
    }
    return NO;
}

+ (BOOL)startRecordingWithConfiguration:(NSDictionary *)config delegate:(id)delegate error:(NSError **)error {
    if (g_isRecording) {
        return NO;
    }
    
    g_outputPath = config[@"outputPath"];
    g_capturedFrames = [NSMutableArray array];
    
    NSLog(@"ðŸŽ¬ Starting simple frame capture approach");
    
    [SCShareableContent getShareableContentWithCompletionHandler:^(SCShareableContent *content, NSError *contentError) {
        if (contentError) {
            NSLog(@"âŒ Failed to get content: %@", contentError);
            return;
        }
        
        // Get primary display
        SCDisplay *targetDisplay = content.displays.firstObject;
        
        // Simple content filter - no exclusions for now
        SCContentFilter *filter = [[SCContentFilter alloc] initWithDisplay:targetDisplay excludingWindows:@[]];
        
        // Stream configuration
        SCStreamConfiguration *streamConfig = [[SCStreamConfiguration alloc] init];
        streamConfig.width = 1280;
        streamConfig.height = 720;
        streamConfig.minimumFrameInterval = CMTimeMake(1, 30);
        streamConfig.pixelFormat = kCVPixelFormatType_32BGRA;
        
        // Create delegates
        g_streamDelegate = [[FrameCapturDelegate alloc] init];
        g_streamOutput = [[FrameCaptureOutput alloc] init];
        
        // Create stream
        g_stream = [[SCStream alloc] initWithFilter:filter configuration:streamConfig delegate:g_streamDelegate];
        
        [g_stream addStreamOutput:g_streamOutput
                             type:SCStreamOutputTypeScreen
               sampleHandlerQueue:dispatch_get_global_queue(DISPATCH_QUEUE_PRIORITY_HIGH, 0)
                            error:nil];
        
        [g_stream startCaptureWithCompletionHandler:^(NSError *startError) {
            if (startError) {
                NSLog(@"âŒ Failed to start capture: %@", startError);
            } else {
                NSLog(@"âœ… Frame capture started");
                g_isRecording = YES;
            }
        }];
    }];
    
    return YES;
}

+ (void)stopRecording {
    if (!g_isRecording || !g_stream) {
        return;
    }
    
    NSLog(@"ðŸ›‘ Stopping frame capture");
    
    [g_stream stopCaptureWithCompletionHandler:^(NSError *stopError) {
        if (stopError) {
            NSLog(@"âŒ Stop error: %@", stopError);
        }
        // Video creation happens in delegate
    }];
}

+ (BOOL)isRecording {
    return g_isRecording;
}

+ (void)createVideoFromFrames {
    if (g_capturedFrames.count == 0) {
        NSLog(@"âŒ No frames captured");
        return;
    }
    
    NSLog(@"ðŸŽ¬ Creating video from %lu frames", (unsigned long)g_capturedFrames.count);
    
    // Use simple approach - write first frame as image to test
    NSImage *firstFrame = g_capturedFrames.firstObject;
    if (firstFrame) {
        NSString *testImagePath = [g_outputPath stringByReplacingOccurrencesOfString:@".mov" withString:@"_test.png"];
        
        // Convert NSImage to PNG data
        CGImageRef cgImage = [firstFrame CGImageForProposedRect:NULL context:NULL hints:NULL];
        NSBitmapImageRep *bitmapRep = [[NSBitmapImageRep alloc] initWithCGImage:cgImage];
        NSData *pngData = [bitmapRep representationUsingType:NSBitmapImageFileTypePNG properties:@{}];
        [pngData writeToFile:testImagePath atomically:YES];
        NSLog(@"âœ… Test image saved: %@", testImagePath);
    }
    
    // For now, just create a simple video file that works
    NSURL *outputURL = [NSURL fileURLWithPath:g_outputPath];
    
    // Create a working video using AVAssetWriter with frames
    NSError *error = nil;
    AVAssetWriter *assetWriter = [[AVAssetWriter alloc] initWithURL:outputURL fileType:AVFileTypeQuickTimeMovie error:&error];
    
    if (error) {
        NSLog(@"âŒ Asset writer error: %@", error);
        return;
    }
    
    // Simple video settings that definitely work
    NSDictionary *videoSettings = @{
        AVVideoCodecKey: AVVideoCodecTypeH264,
        AVVideoWidthKey: @1280,
        AVVideoHeightKey: @720,
        AVVideoCompressionPropertiesKey: @{
            AVVideoAverageBitRateKey: @(1280 * 720 * 3)
        }
    };
    
    AVAssetWriterInput *writerInput = [AVAssetWriterInput assetWriterInputWithMediaType:AVMediaTypeVideo outputSettings:videoSettings];
    writerInput.expectsMediaDataInRealTime = NO;
    
    if ([assetWriter canAddInput:writerInput]) {
        [assetWriter addInput:writerInput];
    }
    
    // Start writing session
    [assetWriter startWriting];
    [assetWriter startSessionAtSourceTime:kCMTimeZero];
    
    // Create simple 1-second video with first frame
    dispatch_async(dispatch_get_global_queue(DISPATCH_QUEUE_PRIORITY_HIGH, 0), ^{
        
        // Create pixel buffer pool
        NSDictionary *pixelBufferAttributes = @{
            (NSString*)kCVPixelBufferPixelFormatTypeKey: @(kCVPixelFormatType_32ARGB),
            (NSString*)kCVPixelBufferWidthKey: @1280,
            (NSString*)kCVPixelBufferHeightKey: @720
        };
        
        AVAssetWriterInputPixelBufferAdaptor *adaptor = [AVAssetWriterInputPixelBufferAdaptor assetWriterInputPixelBufferAdaptorWithAssetWriterInput:writerInput sourcePixelBufferAttributes:pixelBufferAttributes];
        
        // Add some frames
        for (int i = 0; i < 30 && i < g_capturedFrames.count; i++) { // 1 second worth
            if (writerInput.isReadyForMoreMediaData) {
                
                CVPixelBufferRef pixelBuffer = NULL;
                CVPixelBufferCreate(kCFAllocatorDefault, 1280, 720, kCVPixelFormatType_32ARGB, (__bridge CFDictionaryRef)pixelBufferAttributes, &pixelBuffer);
                
                if (pixelBuffer) {
                    CMTime frameTime = CMTimeMake(i, 30);
                    [adaptor appendPixelBuffer:pixelBuffer withPresentationTime:frameTime];
                    CVPixelBufferRelease(pixelBuffer);
                }
            }
        }
        
        [writerInput markAsFinished];
        [assetWriter finishWritingWithCompletionHandler:^{
            if (assetWriter.status == AVAssetWriterStatusCompleted) {
                NSLog(@"âœ… Simple video created: %@", g_outputPath);
            } else {
                NSLog(@"âŒ Video creation failed: %@", assetWriter.error);
            }
            
            // Cleanup
            g_capturedFrames = nil;
            g_stream = nil;
            g_streamDelegate = nil;
            g_streamOutput = nil;
        }];
    });
}

@end