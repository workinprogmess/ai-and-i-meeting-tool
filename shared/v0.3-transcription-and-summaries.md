# v0.3 transcription & summaries plan

## vision
deliver a rock-solid transcription workflow and a first version of our human, context-aware summary surface. audio should flow automatically from recording to transcripts across all services, and the app should give users the clearest possible view of what happened in their meeting.

## goals
- make the transcription pipeline resilient, observable, and fast enough for daily use.
- store transcripts in a canonical structure so future features (search, sharing, corrections) are straightforward.
- ship richer status + confidence feedback in the ui so users instantly trust the output.
- introduce a flexible summary generator that produces relevant, human-sounding narratives tailored to each meeting.

## immediate todo queue
1. `audio-prep` – build the ffmpeg presence check and single-retry conversion wrapper; return structured errors to the ui.
2. `canonical-store` – draft the `session_<id>_transcripts.json` schema and implement atomic writes from the transcription coordinator.
3. `service-prompts` – refresh prompt payloads across providers with multilingual hints, speaker roles, and device context.
4. `status-surface` – emit queued/running/success/failure states from each service and thread them through the ui toast/log view.
5. `confidence-model` – normalize confidence scores from all providers and persist them alongside transcript segments.
6. `confidence-ui` – add the opacity + warning icon treatment for low confidence entries, including tooltip copy.
7. `summary-engine` – stand up the summary input contract and connect the model orchestration layer (initial heuristic prompts ok).
8. `telemetry-pass` – extend `performancemonitor` to log per-service durations, retries, selected best-service id, and summary latency.

## workstreams & tasks

### audio preparation & upload
- **ffmpeg availability check** – detect `ffmpeg` before conversion, warn early, and skip unnecessary work when the tool is missing.
- **adaptive mp3 conversion** – pick bitrate bands (≤30 min → 128 kbps, 30–60 min → 96 kbps, >60 min → 64 kbps) to keep files under provider limits.
- **single-retry conversion** – wrap conversion in a backoff retry and surface precise error messages back to the ui when it fails.
- **size guard + chunking** – measure the output file; if it still exceeds a service cap, fall into the chunked upload path (10 min slices with overlap) before transcription.

### canonical transcript pipeline
- **metadata contract** – persist transcripts under `session_<id>_transcripts.json` with a top-level index of services and “best” choice.
- **atomic writes** – write transcript files via temporary locations + rename so concurrent sessions never clobber each other.
- **service prompts** – update request payloads to use multilingual hints, speaker context, and device routing notes drawn from recordings.
- **status updates** – have each service emit queued → running → success/failure states with missing-key diagnostics where relevant.
- **confidence capture** – parse confidence scores (words or segments) from every provider and normalize to 0.0–1.0.

### application experience
- **confidence display** – surface low confidence lines with subtle opacity shifts and a warning icon; include tooltip text.
- **transcript selector** – reflect the canonical storage by allowing quick switching between services while remembering the preferred one.

### summary generation
- **summary input contract** – define a shared structure (agenda, highlights, risks, follow-ups) that feeds the summary generator.
- **model orchestration** – wire the ai service(s) to produce summaries focused on relevance and human clarity (no rigid “sally rooney” style; adapt tone to meeting type and user preference).
- **quality guardrails** – add length caps, fallback prompts, and retry rules so summaries are dependable even when transcripts are sparse.

### validation & tooling
- **integration tests** – create scripted runs that execute mix → convert → transcribe → summarise using fixture audio.
- **telemetry extensions** – record per-service durations, retries, and selected best-service identifiers in `performancemonitor`.

## dependencies & references
- transcript formatting cues (emotion, sentiment, structure): `shared/transcript-format-sample.md`
- updated roadmap context: `shared/native_app_implementation_plan.md` (v0.3 section)

## definition of done
- all services transcribe automatically, update their status in real time, and write canonical json without collisions.
- ui exposes service selection, confidence visualization, and precise error states.
- summary generation produces human-readable, context-aware narratives for test meetings on demand.
- integration script(s) pass on fresh recordings and the telemetry dashboard displays per-service stats.
