# v0.3 transcription & summaries plan

## vision
deliver a rock-solid transcription workflow and a first version of our human, context-aware summary surface. audio should flow automatically from recording to transcripts across all services, and the app should give users the clearest possible view of what happened in their meeting.

## goals
- make the transcription pipeline resilient, observable, and fast enough for daily use.
- store transcripts in a canonical structure so future features (search, sharing, corrections) are straightforward.
- ship richer status + confidence feedback in the ui so users instantly trust the output.
- introduce a flexible summary generator that produces relevant, human-sounding narratives tailored to each meeting.

## workstreams & tasks

### audio preparation & upload
- **ffmpeg availability check** – detect `ffmpeg` before conversion, warn early, and skip unnecessary work when the tool is missing.
- **mp3 conversion with retry** – wrap conversion in a single backoff retry and surface precise error messages back to the ui.

### canonical transcript pipeline
- **metadata contract** – persist transcripts under `session_<id>_transcripts.json` with a top-level index of services and “best” choice.
- **atomic writes** – write transcript files via temporary locations + rename so concurrent sessions never clobber each other.
- **service prompts** – update request payloads to use multilingual hints, speaker context, and device routing notes drawn from recordings.
- **status updates** – have each service emit queued → running → success/failure states with missing-key diagnostics where relevant.
- **confidence capture** – parse confidence scores (words or segments) from every provider and normalize to 0.0–1.0.

### application experience
- **confidence display** – surface low confidence lines with subtle opacity shifts and a warning icon; include tooltip text.
- **transcript selector** – reflect the canonical storage by allowing quick switching between services while remembering the preferred one.

### summary generation
- **summary input contract** – define a shared structure (agenda, highlights, risks, follow-ups) that feeds the summary generator.
- **model orchestration** – wire the ai service(s) to produce summaries focused on relevance and human clarity (no rigid “sally rooney” style; adapt tone to meeting type and user preference).
- **quality guardrails** – add length caps, fallback prompts, and retry rules so summaries are dependable even when transcripts are sparse.

### validation & tooling
- **integration tests** – create scripted runs that execute mix → convert → transcribe → summarise using fixture audio.
- **telemetry extensions** – record per-service durations, retries, and selected best-service identifiers in `performancemonitor`.

## dependencies & references
- previous transcription deep dive: `shared/milestone_2_transcription_plan.md`
- updated roadmap context: `shared/native_app_implementation_plan.md` (v0.3 section)

## definition of done
- all services transcribe automatically, update their status in real time, and write canonical json without collisions.
- ui exposes service selection, confidence visualization, and precise error states.
- summary generation produces human-readable, context-aware narratives for test meetings on demand.
- integration script(s) pass on fresh recordings and the telemetry dashboard displays per-service stats.
